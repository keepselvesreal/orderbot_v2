{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory 공유 확인 위한 dummy chain 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "\n",
      "            You are a robot that classifies customer input messages into specific types. If order_id is provided, you need to output '조회'.\n",
      "            If order_id is not provided, you need to classify the customer input message into one of the following two types:\n",
      "            - 상품 문의, 주문 내역 조회, 주문 변경 내역 조회, 주문 취소 내역 조회: '문의'\n",
      "            - 주문 요청, 주문 변경 요청, 주문 취소 요청: '요청'\n",
      "            \n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{chat_history}\u001b[0m\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "input:\u001b[33;1m\u001b[1;3m{input}\u001b[0m\n",
      "order_id:\u001b[33;1m\u001b[1;3m{order_id}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "dummy_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types. If order_id is provided, you need to output '조회'.\n",
    "            If order_id is not provided, you need to classify the customer input message into one of the following two types:\n",
    "            - 상품 문의, 주문 내역 조회, 주문 변경 내역 조회, 주문 취소 내역 조회: '문의'\n",
    "            - 주문 요청, 주문 변경 요청, 주문 취소 요청: '요청'\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"input:{input}\\norder_id:{order_id}\"),\n",
    "    ]\n",
    ")\n",
    "dummy_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_chain = dummy_prompt | model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "dumy_chain_with_memory = RunnableWithMessageHistory(\n",
    "    dummy_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run a727ab67-dfc3-4e5b-bd65-802e9f19b9b0 not found for run cec82afd-a02a-4857-8e4f-5c176776f5cd. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 148, 'total_tokens': 151}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5af9ca04-d147-47a9-9c23-5f3f258128cb-0')"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = dumy_chain_with_memory.invoke(\n",
    "    {\"input\": \"주문 변경하고 싶어\", \n",
    "    \"order_id\": None},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240517-2\"}}\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 취소하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 150, 'total_tokens': 153}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-da099424-affc-4335-b10e-8c6bb4093e93-0'),\n",
       " HumanMessage(content='없음'),\n",
       " AIMessage(content='주문 취소', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 89, 'total_tokens': 94}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d3e2950d-3111-40bc-8ff3-7a1ac5996cf3-0'),\n",
       " HumanMessage(content='없음'),\n",
       " AIMessage(content='no', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 292, 'total_tokens': 293}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0aeac858-9413-4f2a-821e-f779e312e018-0')]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240517-1\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 변경하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 148, 'total_tokens': 151}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5af9ca04-d147-47a9-9c23-5f3f258128cb-0')]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240517-2\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주문 변경/취소 분류 체인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이젠 필요 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = store[\"test_240516-1\"].messages\n",
    "chat_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            you need to review the chat_history from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'order_id'], template='input:{input}\\norder_id:{order_id}'))])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "order_change_cancel_prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types. \n",
    "            you need to review the chat_history from the latest to the oldest and output either '주문 변경' or '주문 취소'.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"input:{input}\\norder_id:{order_id}\"),\n",
    "    ]\n",
    ")\n",
    "order_change_cancel_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프롬프트 수정\n",
    "* You need to review the messages in the Messages Placeholder from the latest to the oldest and output either '주문 변경' or '주문 취소'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            You need to review the messages in the Messages Placeholder from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'order_id'], template='input:{input}\\norder_id:{order_id}'))])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "order_change_cancel_prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types. \n",
    "            You need to review the messages in the Messages Placeholder from the latest to the oldest and output either '주문 변경' or '주문 취소'.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"input:{input}\\norder_id:{order_id}\"),\n",
    "    ]\n",
    ")\n",
    "order_change_cancel_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프롬프트 수정2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            You need to review the messages in the Messages Placeholder from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n\\n            Classify as '주문 변경' if there are any indications or keywords related to modifying an order, such as:\\n            - 변경\\n            - 수정\\n            - 바꾸다\\n            - 변경하고 싶어\\n\\n            Classify as '주문 취소' if there are any indications or keywords related to canceling an order, such as:\\n            - 취소\\n            - 취소하고 싶어\\n            - 취소 요청\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'order_id'], template='input:{input}\\norder_id:{order_id}'))])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "order_change_cancel_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types. \n",
    "            You need to review the messages in the Messages Placeholder from the latest to the oldest and output either '주문 변경' or '주문 취소'.\n",
    "\n",
    "            Classify as '주문 변경' if there are any indications or keywords related to modifying an order, such as:\n",
    "            - 변경\n",
    "            - 수정\n",
    "            - 바꾸다\n",
    "            - 변경하고 싶어\n",
    "\n",
    "            Classify as '주문 취소' if there are any indications or keywords related to canceling an order, such as:\n",
    "            - 취소\n",
    "            - 취소하고 싶어\n",
    "            - 취소 요청\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"input:{input}\\norder_id:{order_id}\"),\n",
    "    ]\n",
    ")\n",
    "order_change_cancel_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            You need to review the messages in the Messages Placeholder from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n\\n            Classify as '주문 변경' if there are any indications or keywords related to modifying an order, such as:\\n            - 변경\\n            - 수정\\n            - 바꾸다\\n            - 변경하고 싶어\\n\\n            Classify as '주문 취소' if there are any indications or keywords related to canceling an order, such as:\\n            - 취소\\n            - 취소하고 싶어\\n            - 취소 요청\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'order_id'], template='input:{input}\\norder_id:{order_id}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001C1117B1B40>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001C1100FF400>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_change_or_cancel_chain = order_change_cancel_prompt | model\n",
    "classify_change_or_cancel_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오늘 추가 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "classify_change_or_cancel_chain_with_memory = RunnableWithMessageHistory(\n",
    "    classify_change_or_cancel_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최초 지정 session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(lambda x: classify_change_or_cancel_chain_with_memory.invoke(x, config={'configurable': {'session_id': 'test_240517-1'}}))"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "classify_change_or_cancel_chain_with_memory_lambda = RunnableLambda(\n",
    "    lambda x: classify_change_or_cancel_chain_with_memory.invoke(x,\n",
    "                                                                 config={\"configurable\": {\"session_id\": \"test_240517-1\"}})\n",
    ")\n",
    "classify_change_or_cancel_chain_with_memory_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "session_id 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(lambda x: classify_change_or_cancel_chain_with_memory.invoke(x, config={'configurable': {'session_id': 'test_240517-2'}}))"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "classify_change_or_cancel_chain_with_memory_lambda = RunnableLambda(\n",
    "    lambda x: classify_change_or_cancel_chain_with_memory.invoke(x,\n",
    "                                                                 config={\"configurable\": {\"session_id\": \"test_240517-2\"}})\n",
    ")\n",
    "classify_change_or_cancel_chain_with_memory_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 승인 메시지 식별 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['action_type', 'chat_history', 'input', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            You need to determine whether the user has responded to the AI's request for approval regarding the action specified in action_type.\\n            \\n            To make this determination, the following conditions must be met:\\n            1. There must be an AIMessage that asks for approval for the action specified in action_type, based on the specified order details.\\n            2. The user must have explicitly expressed consent in response to the AIMessage asking for approval of the action specified in action_type.\\n            \\n            Your response must be either 'yes' or 'no'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['action_type', 'input', 'order_id'], template='input:{input}\\norder_id:{order_id}\\naction_type: {action_type}'))])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "classify_confirmation_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types. \n",
    "            You need to determine whether the user has responded to the AI's request for approval regarding the action specified in action_type.\n",
    "            \n",
    "            To make this determination, the following conditions must be met:\n",
    "            1. There must be an AIMessage that asks for approval for the action specified in action_type, based on the specified order details.\n",
    "            2. The user must have explicitly expressed consent in response to the AIMessage asking for approval of the action specified in action_type.\n",
    "            \n",
    "            Your response must be either 'yes' or 'no'.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"input:{input}\\norder_id:{order_id}\\naction_type: {action_type}\"),\n",
    "    ]\n",
    ")\n",
    "classify_confirmation_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['action_type', 'chat_history', 'input', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            You need to determine whether the user has responded to the AI's request for approval regarding the action specified in action_type.\\n            \\n            To make this determination, the following conditions must be met:\\n            1. There must be an AIMessage that asks for approval for the action specified in action_type, based on the specified order details.\\n            2. The user must have explicitly expressed consent in response to the AIMessage asking for approval of the action specified in action_type.\\n            \\n            Your response must be either 'yes' or 'no'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['action_type', 'input', 'order_id'], template='input:{input}\\norder_id:{order_id}\\naction_type: {action_type}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001C1117B1B40>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001C1100FF400>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_confirmation_chain = classify_confirmation_prompt | model \n",
    "classify_confirmation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "classify_confirmation_chain_with_memory = RunnableWithMessageHistory(\n",
    "    classify_confirmation_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최초 지정 session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(lambda x: classify_confirmation_chain_with_memory.invoke(x, config={'configurable': {'session_id': 'test_240517-1'}}))"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "classify_confirmation_chain_with_memory_lambda = RunnableLambda(\n",
    "    lambda x: classify_confirmation_chain_with_memory.invoke(x,\n",
    "                                                             config={\"configurable\": {\"session_id\": \"test_240517-1\"}})\n",
    ")\n",
    "classify_confirmation_chain_with_memory_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "session_id 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(lambda x: classify_confirmation_chain_with_memory.invoke(x, config={'configurable': {'session_id': 'test_240517-2'}}))"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "classify_confirmation_chain_with_memory_lambda = RunnableLambda(\n",
    "    lambda x: classify_confirmation_chain_with_memory.invoke(x,\n",
    "                                                             config={\"configurable\": {\"session_id\": \"test_240517-2\"}})\n",
    ")\n",
    "classify_confirmation_chain_with_memory_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## action_type 추가 위한 보조 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(dict):\n",
    "    inputs = dict[\"inputs\"]\n",
    "    action_type =  dict[\"action_type\"]\n",
    "    # action_type = action_type.content\n",
    "    inputs[\"action_type\"] = action_type\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 종합-입력에 승인 메시지 포함 여부까지 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  inputs: RunnablePassthrough(),\n",
       "  action_type: ChatPromptTemplate(input_variables=['chat_history', 'input', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            You need to review the chat_history from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n\\n            Classify as '주문 변경' if there are any indications or keywords related to modifying an order, such as:\\n            - 변경\\n            - 수정\\n            - 바꾸다\\n            - 변경하고 싶어\\n\\n            Classify as '주문 취소' if there are any indications or keywords related to canceling an order, such as:\\n            - 취소\\n            - 취소하고 싶어\\n            - 취소 요청\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'order_id'], template='input:{input}\\norder_id:{order_id}'))])\n",
       "               | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001C1117B1B40>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001C1100FF400>, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "}\n",
       "| RunnableLambda(helper)\n",
       "| RunnableAssign(mapper={\n",
       "    execution_confirmation: ChatPromptTemplate(input_variables=['action_type', 'chat_history', 'input', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            You need to determine whether the user has responded to the AI's request for approval regarding the action specified in action_type.\\n            \\n            To make this determination, the following conditions must be met:\\n            1. There must be an AIMessage that asks for approval for the action specified in action_type, based on the specified order details.\\n            2. The user must have explicitly expressed consent in response to the AIMessage asking for approval of the action specified in action_type.\\n            \\n            Your response must be either 'yes' or 'no'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['action_type', 'input', 'order_id'], template='input:{input}\\norder_id:{order_id}\\naction_type: {action_type}'))])\n",
       "                            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001C1117B1B40>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001C1100FF400>, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "  })"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "confirmation_chain = (\n",
    "    {\n",
    "        \"inputs\": RunnablePassthrough(),\n",
    "        \"action_type\": classify_change_or_cancel_chain\n",
    "    } | RunnableLambda(helper) | RunnablePassthrough.assign(execution_confirmation=classify_confirmation_chain)\n",
    "    )\n",
    "confirmation_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수정 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  inputs: RunnablePassthrough(),\n",
       "  action_type: RunnableLambda(lambda x: classify_change_or_cancel_chain_with_memory.invoke(x, config={'configurable': {'session_id': 'test_240517-2'}}))\n",
       "}\n",
       "| RunnableLambda(helper)\n",
       "| RunnableAssign(mapper={\n",
       "    execution_confirmation: RunnableLambda(lambda x: classify_confirmation_chain_with_memory.invoke(x, config={'configurable': {'session_id': 'test_240517-2'}}))\n",
       "  })"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "confirmation_chain = (\n",
    "    {\n",
    "        \"inputs\": RunnablePassthrough(),\n",
    "        \"action_type\": classify_change_or_cancel_chain_with_memory_lambda\n",
    "    } \n",
    "    | RunnableLambda(helper) \n",
    "    | RunnablePassthrough.assign(execution_confirmation=classify_confirmation_chain_with_memory_lambda)\n",
    "    )\n",
    "confirmation_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chat_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[272], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m response \u001b[38;5;241m=\u001b[39m confirmation_chain\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m----> 2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mchat_history\u001b[49m,\n\u001b[0;32m      3\u001b[0m      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m7\u001b[39m}\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m response\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chat_history' is not defined"
     ]
    }
   ],
   "source": [
    "response = confirmation_chain.invoke(\n",
    "    {\"chat_history\": chat_history,\n",
    "     \"input\": None,\n",
    "     \"order_id\": 7}\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수정 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 변경하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 148, 'total_tokens': 151}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5af9ca04-d147-47a9-9c23-5f3f258128cb-0')]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240517-2\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run ff430757-7942-4fd6-b5a6-0fafe419fb19 not found for run 28fbfe1e-c329-49eb-b134-956b1778645b. Treating as a root run.\n",
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got None.')\n",
      "Parent run 1764daaf-56eb-40f5-aa4f-49582b93c49e not found for run 1a2a1d1c-b058-42d3-8d24-e828bc4b868c. Treating as a root run.\n",
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got None.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': None,\n",
       " 'order_id': 7,\n",
       " 'action_type': AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 193, 'total_tokens': 196}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ceea037a-8846-4d02-9c22-f0c41c4f6fdb-0'),\n",
       " 'execution_confirmation': AIMessage(content='no', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 269, 'total_tokens': 270}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-36b741ed-50b6-4c75-a9c5-d797c8e39fae-0')}"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = confirmation_chain.invoke(\n",
    "    {\"input\": None,\n",
    "     \"order_id\": 7}\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 변경하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 148, 'total_tokens': 151}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5af9ca04-d147-47a9-9c23-5f3f258128cb-0')]"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240517-2\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 승인 메시지 작성 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['action_type', 'queried_result'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\n            You are a robot that generates a confirmation request message based on the action_type and queried_result.\\n            create a message to show the queried_result and ask for final confirmation considering action_type.\\n            The response should be generated in Korean.\\n            ')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['action_type', 'queried_result'], template='action_type:{action_type}\\nqueried_result:{queried_result}'))])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "generate_confirm_message_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that generates a confirmation request message based on the action_type and queried_result.\n",
    "            create a message to show the queried_result and ask for final confirmation considering action_type.\n",
    "            The response should be generated in Korean.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        # MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"action_type:{action_type}\\nqueried_result:{queried_result}\")\n",
    "    ]\n",
    ")\n",
    "generate_confirm_message_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['action_type', 'queried_result'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\n            You are a robot that generates a confirmation request message based on the action_type and queried_result.\\n            create a message to show the queried_result and ask for final confirmation considering action_type.\\n            The response should be generated in Korean.\\n            ')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['action_type', 'queried_result'], template='action_type:{action_type}\\nqueried_result:{queried_result}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001C1117B1B40>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001C1100FF400>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_confirm_message_chain = generate_confirm_message_prompt | model\n",
    "generate_confirm_message_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "generate_confirm_message_chain_with_memory = RunnableWithMessageHistory(\n",
    "    generate_confirm_message_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(lambda x: generate_confirm_message_chain_with_memory.invoke(x, config={'configurable': {'session_id': 'test_240517-2'}}))"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "generate_confirm_message_chain_with_memory_lambda = RunnableLambda(\n",
    "    lambda x: generate_confirm_message_chain_with_memory.invoke(x,\n",
    "                                                             config={\"configurable\": {\"session_id\": \"test_240517-2\"}})\n",
    ")\n",
    "generate_confirm_message_chain_with_memory_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일단 나중에 구현하기로 보류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def enrichMemoryContext(session_id, input, context, chain_with_memory):\n",
    "    RunnableLambda(\n",
    "        chain_with_memory.invoke(input,\n",
    "                                 config={\"configurable\": {\"session_id\": session_id}})\n",
    "                                 )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주문 조회 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_recent_orders(dict):\n",
    "    print(dict)\n",
    "    return \"produdct: 백설기 quantity: 2 price: 13,000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 승인 메시지 포함 여부에 따른 라우트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution_or_message_route(info):\n",
    "    print(\"execution_or_message_route 함수로 전달된 데이터 -> \", info)\n",
    "    if \"yes\" in info[\"execution_confirmation\"].content.lower():\n",
    "        return \"주문 변경 또는 취소 진행\"\n",
    "    else:\n",
    "        return RunnablePassthrough.assign(queried_result=fetch_recent_orders) | generate_confirm_message_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수정 버전\n",
    "* generate_confirm_message_chain_with_memory_lambda 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution_or_message_route(info):\n",
    "    print(\"execution_or_message_route 함수로 전달된 데이터 -> \", info)\n",
    "    if \"yes\" in info[\"execution_confirmation\"].content.lower():\n",
    "        return \"주문 변경 또는 취소 진행\"\n",
    "    else:\n",
    "        return RunnablePassthrough.assign(queried_result=fetch_recent_orders) | generate_confirm_message_chain_with_memory_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "order_change_chain 동작 살펴보기 위해 임시 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution_or_message_route(info):\n",
    "    print(\"execution_or_message_route 함수로 전달된 데이터 -> \", info)\n",
    "    if \"yes\" in info[\"execution_confirmation\"].content.lower():\n",
    "        return order_change_chain_with_memory_lambda\n",
    "    else:\n",
    "        return RunnablePassthrough.assign(queried_result=fetch_recent_orders) | generate_confirm_message_chain_with_memory_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  inputs: RunnablePassthrough(),\n",
       "  action_type: RunnableLambda(lambda x: classify_change_or_cancel_chain_with_memory.invoke(x, config={'configurable': {'session_id': 'test_240517-2'}}))\n",
       "}\n",
       "| RunnableLambda(helper)\n",
       "| RunnableAssign(mapper={\n",
       "    execution_confirmation: RunnableLambda(lambda x: classify_confirmation_chain_with_memory.invoke(x, config={'configurable': {'session_id': 'test_240517-2'}}))\n",
       "  })\n",
       "| RunnableLambda(execution_or_message_route)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution_chain = confirmation_chain | execution_or_message_route\n",
    "execution_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run d66376b3-a137-4724-b6a2-81dcecf4f7d9 not found for run 9e7e916a-56c9-4feb-b7f8-f581f863ca62. Treating as a root run.\n",
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got None.')\n",
      "Parent run de4989da-0619-480b-a754-a7f49249fd37 not found for run 805072c6-40c1-499c-8fd6-ba32501270b7. Treating as a root run.\n",
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got None.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution_or_message_route 함수로 전달된 데이터 ->  {'input': None, 'order_id': 7, 'action_type': AIMessage(content='주문 취소', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 87, 'total_tokens': 92}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5a4068af-71b3-4717-bc28-f36d058716fa-0'), 'execution_confirmation': AIMessage(content='no', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 271, 'total_tokens': 272}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-85d2c44c-88d9-431a-8dce-06771bba1b64-0')}\n",
      "{'input': None, 'order_id': 7, 'action_type': AIMessage(content='주문 취소', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 87, 'total_tokens': 92}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5a4068af-71b3-4717-bc28-f36d058716fa-0'), 'execution_confirmation': AIMessage(content='no', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 271, 'total_tokens': 272}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-85d2c44c-88d9-431a-8dce-06771bba1b64-0')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='주문 내역 확인 - 제품: 백설기, 수량: 2개, 가격: 13,000원\\n주문을 취소하시겠습니까?', response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 185, 'total_tokens': 236}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-11c03f97-76da-4f0e-88e9-4d32ee8e389f-0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = execution_chain.invoke(\n",
    "    {\"input\": None,\n",
    "     \"order_id\": 7}\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 취소하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 150, 'total_tokens': 153}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-da099424-affc-4335-b10e-8c6bb4093e93-0')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240517-1\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 83b77107-a9ea-4bfd-8e6e-1254aad8a0ec not found for run f60e9d39-9b5a-4481-8102-84376e0e87ba. Treating as a root run.\n",
      "Parent run 4e005ac1-ea5b-41b8-bb2f-a3b7eb458bb4 not found for run 0f75ec39-4ff4-41a3-9efa-00baa3357196. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution_or_message_route 함수로 전달된 데이터 ->  {'input': '응', 'order_id': 7, 'action_type': AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 86, 'total_tokens': 89}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-dfc8a994-0e1a-48d7-aa26-71dfa5c23de1-0'), 'execution_confirmation': AIMessage(content='yes', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 284, 'total_tokens': 285}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b1569b47-166c-4969-a53e-ead7c9f5f1af-0')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'주문 변경 또는 취소 진행'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = execution_chain.invoke(\n",
    "    {\"input\": \"응\",\n",
    "     \"order_id\": 7}\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 4c281470-0b18-4e1b-b13b-20e5c14f29a1 not found for run 6694aff6-23af-4320-ac57-562af4c265e3. Treating as a root run.\n",
      "Parent run 9db37b8e-6eaa-4739-8c81-3a0121ea67c2 not found for run 16ef045e-078e-4339-93d9-25c5febe9048. Treating as a root run.\n",
      "Parent run 188072dd-027a-4275-81e2-94af2c670740 not found for run d6e7fd5b-3d68-4dc0-aceb-23f5acea619a. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution_or_message_route 함수로 전달된 데이터 ->  {'input': '없음', 'order_id': 3, 'action_type': AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 195, 'total_tokens': 198}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-703fcf12-aaef-4ac7-9f9c-e64fe176b0a8-0'), 'execution_confirmation': AIMessage(content='no', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 286, 'total_tokens': 287}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-808420a1-40f8-44d6-8e9b-0f0c576466c1-0')}\n",
      "{'input': '없음', 'order_id': 3, 'action_type': AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 195, 'total_tokens': 198}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-703fcf12-aaef-4ac7-9f9c-e64fe176b0a8-0'), 'execution_confirmation': AIMessage(content='no', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 286, 'total_tokens': 287}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-808420a1-40f8-44d6-8e9b-0f0c576466c1-0')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='주문 내용을 확인해 주세요.\\n\\n상품: 백설기\\n수량: 2\\n가격: 13,000\\n\\n주문을 변경하시겠습니까? 최종 확인 부탁드립니다.', response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 186, 'total_tokens': 249}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b18fa125-014b-4304-a4e7-e005697dfff4-0')"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = execution_chain.invoke(\n",
    "    {\"input\": '없음',\n",
    "     \"order_id\": 3}\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 대화의 주문 변공하고 싶어 내용 보고도 멍청하게 '주문 취소'로 분류했던 적 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 변경하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 148, 'total_tokens': 151}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-5af9ca04-d147-47a9-9c23-5f3f258128cb-0'),\n",
       " HumanMessage(content='없음'),\n",
       " AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 195, 'total_tokens': 198}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-703fcf12-aaef-4ac7-9f9c-e64fe176b0a8-0'),\n",
       " HumanMessage(content='없음'),\n",
       " AIMessage(content='no', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 286, 'total_tokens': 287}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-808420a1-40f8-44d6-8e9b-0f0c576466c1-0'),\n",
       " HumanMessage(content='없음'),\n",
       " AIMessage(content='주문 내용을 확인해 주세요.\\n\\n상품: 백설기\\n수량: 2\\n가격: 13,000\\n\\n주문을 변경하시겠습니까? 최종 확인 부탁드립니다.', response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 186, 'total_tokens': 249}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b18fa125-014b-4304-a4e7-e005697dfff4-0')]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240517-2\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run c4506944-84df-467d-8f27-f28087251bc7 not found for run c5c82310-3888-4dae-9156-5932c3fba8c5. Treating as a root run.\n",
      "Parent run dadcc70c-f762-4981-aa8d-94443d8ce5dc not found for run 181ca3b5-4dc7-41df-831a-26303ea8fe07. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution_or_message_route 함수로 전달된 데이터 ->  {'input': '응', 'order_id': 3, 'action_type': AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 294, 'total_tokens': 297}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ad5ec986-90ab-43e0-b34b-ed2d453d4dd8-0'), 'execution_confirmation': AIMessage(content='yes', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 381, 'total_tokens': 382}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c16f070f-78fa-4100-8be2-9d2d742942dd-0')}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'order_change_chain_with_memory_lambda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[322], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mexecution_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m응\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m     \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morder_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m response\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3963\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3961\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[39;00m\n\u001b[0;32m   3962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 3963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m   3964\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke,\n\u001b[0;32m   3965\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3966\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config(config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc),\n\u001b[0;32m   3967\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3968\u001b[0m     )\n\u001b[0;32m   3969\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   3971\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3972\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3973\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:1626\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1623\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[0;32m   1624\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1625\u001b[0m         Output,\n\u001b[1;32m-> 1626\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1627\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1629\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1630\u001b[0m             config,\n\u001b[0;32m   1631\u001b[0m             run_manager,\n\u001b[0;32m   1632\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1633\u001b[0m         ),\n\u001b[0;32m   1634\u001b[0m     )\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1636\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3837\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   3835\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[0;32m   3836\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3837\u001b[0m     output \u001b[38;5;241m=\u001b[39m call_func_with_variable_args(\n\u001b[0;32m   3838\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   3839\u001b[0m     )\n\u001b[0;32m   3840\u001b[0m \u001b[38;5;66;03m# If the output is a runnable, invoke it\u001b[39;00m\n\u001b[0;32m   3841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[304], line 4\u001b[0m, in \u001b[0;36mexecution_or_message_route\u001b[1;34m(info)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution_or_message_route 함수로 전달된 데이터 -> \u001b[39m\u001b[38;5;124m\"\u001b[39m, info)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution_confirmation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mlower():\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43morder_change_chain_with_memory_lambda\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnablePassthrough\u001b[38;5;241m.\u001b[39massign(queried_result\u001b[38;5;241m=\u001b[39mfetch_recent_orders) \u001b[38;5;241m|\u001b[39m generate_confirm_message_chain_with_memory_lambda\n",
      "\u001b[1;31mNameError\u001b[0m: name 'order_change_chain_with_memory_lambda' is not defined"
     ]
    }
   ],
   "source": [
    "response = execution_chain.invoke(\n",
    "    {\"input\": '응',\n",
    "     \"order_id\": 3}\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 변경하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 148, 'total_tokens': 151}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8f04f2de-8dfe-4b6e-80d1-f30c081d2645-0'),\n",
       " HumanMessage(content='없음'),\n",
       " AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 88, 'total_tokens': 91}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-52a346bd-e424-43f7-a671-cd6aa381f629-0'),\n",
       " HumanMessage(content='없음'),\n",
       " AIMessage(content='no', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 281, 'total_tokens': 282}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-8bf240bb-75c0-46a7-b9b9-d915c59359ce-0'),\n",
       " HumanMessage(content='없음'),\n",
       " AIMessage(content='주문 내용을 변경하려고 합니다. 다음 상품을 주문하였습니다:\\n상품: 백설기\\n수량: 2개\\n가격: 13,000원\\n\\n변경 사항을 최종 확인하시겠습니까?', response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 181, 'total_tokens': 258}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f2f4a4af-ea56-4316-9813-8a89f6a7c249-0'),\n",
       " HumanMessage(content='응'),\n",
       " AIMessage(content='주문 취소', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 201, 'total_tokens': 206}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-14a7ad3c-6c43-4750-908f-677d30d4f929-0'),\n",
       " HumanMessage(content='응'),\n",
       " AIMessage(content='yes', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 402, 'total_tokens': 403}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b938d2cd-3f86-4325-a0b5-f7166a3ccb93-0')]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240517-2\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주문 취소 처리 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancel_order(dict):\n",
    "    order_id = dict[\"order_id\"]\n",
    "    return f\"주문{order_id} 취소\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주문 변경 처리 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "order_change_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a customer service assistant responsible for identifying and processing order changes based on customer chat history.\n",
    "            First, find an AIMessage in the chat history that presents an 'order_id' and the corresponding order details, asking if the customer wishes to change the order.\n",
    "            Then, look for the customer's response to the AIMessage regarding their desired changes. \n",
    "            Combine the AIMessage provided order details and the customer’s input to finalize the order changes.\n",
    "            \n",
    "            If the customer's response indicates that the presented order details are not the ones they want to change, do not proceed with the order change as it indicates a mismatch.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        # MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_change_chain = order_change_prompt | model\n",
    "order_change_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "order_change_chain_with_memory = RunnableWithMessageHistory(\n",
    "    order_change_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_change_chain_with_memory_lambda = RunnableLambda(\n",
    "    lambda x: order_change_chain_with_memory.invoke(x,\n",
    "                                                    config={\"configurable\": {\"session_id\": \"test_240517-2\"}})\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 취소하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 150, 'total_tokens': 153}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-da099424-affc-4335-b10e-8c6bb4093e93-0'),\n",
       " HumanMessage(content='없음'),\n",
       " AIMessage(content='주문 취소', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 89, 'total_tokens': 94}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d3e2950d-3111-40bc-8ff3-7a1ac5996cf3-0'),\n",
       " HumanMessage(content='없음'),\n",
       " AIMessage(content='no', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 292, 'total_tokens': 293}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0aeac858-9413-4f2a-821e-f779e312e018-0')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "store[\"test_240517-1\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 변경하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 148, 'total_tokens': 151}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9a2c12fc-1e47-42e4-8d65-b0513859cc59-0')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "store[\"test_240517-2\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주문 변경/취소 분류 체인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이젠 필요 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = store[\"test_240516-1\"].messages\n",
    "chat_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            you need to review the chat_history from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'order_id'], template='input:{input}\\norder_id:{order_id}'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "order_change_cancel_prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types. \n",
    "            you need to review the chat_history from the latest to the oldest and output either '주문 변경' or '주문 취소'.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"input:{input}\\norder_id:{order_id}\"),\n",
    "    ]\n",
    ")\n",
    "order_change_cancel_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프롬프트 수정\n",
    "* You need to review the messages in the Messages Placeholder from the latest to the oldest and output either '주문 변경' or '주문 취소'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            You need to review the messages in the Messages Placeholder from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'order_id'], template='input:{input}\\norder_id:{order_id}'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "order_change_cancel_prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types. \n",
    "            You need to review the messages in the Messages Placeholder from the latest to the oldest and output either '주문 변경' or '주문 취소'.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"input:{input}\\norder_id:{order_id}\"),\n",
    "    ]\n",
    ")\n",
    "order_change_cancel_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프롬프트 수정2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            You need to review the messages in the Messages Placeholder from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n\\n            Classify as '주문 변경' if there are any indications or keywords related to modifying an order, such as:\\n            - 변경\\n            - 수정\\n            - 바꾸다\\n            - 변경하고 싶어\\n\\n            Classify as '주문 취소' if there are any indications or keywords related to canceling an order, such as:\\n            - 취소\\n            - 취소하고 싶어\\n            - 취소 요청\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'order_id'], template='input:{input}\\norder_id:{order_id}'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "order_change_cancel_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types. \n",
    "            You need to review the messages in the Messages Placeholder from the latest to the oldest and output either '주문 변경' or '주문 취소'.\n",
    "\n",
    "            Classify as '주문 변경' if there are any indications or keywords related to modifying an order, such as:\n",
    "            - 변경\n",
    "            - 수정\n",
    "            - 바꾸다\n",
    "            - 변경하고 싶어\n",
    "\n",
    "            Classify as '주문 취소' if there are any indications or keywords related to canceling an order, such as:\n",
    "            - 취소\n",
    "            - 취소하고 싶어\n",
    "            - 취소 요청\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"input:{input}\\norder_id:{order_id}\"),\n",
    "    ]\n",
    ")\n",
    "order_change_cancel_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            You need to review the messages in the Messages Placeholder from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n\\n            Classify as '주문 변경' if there are any indications or keywords related to modifying an order, such as:\\n            - 변경\\n            - 수정\\n            - 바꾸다\\n            - 변경하고 싶어\\n\\n            Classify as '주문 취소' if there are any indications or keywords related to canceling an order, such as:\\n            - 취소\\n            - 취소하고 싶어\\n            - 취소 요청\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'order_id'], template='input:{input}\\norder_id:{order_id}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001C1117B1B40>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001C1100FF400>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classify_change_or_cancel_chain = order_change_cancel_prompt | model\n",
    "classify_change_or_cancel_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오늘 추가 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "classify_change_or_cancel_chain_with_memory = RunnableWithMessageHistory(\n",
    "    classify_change_or_cancel_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최초 지정 session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(lambda x: classify_change_or_cancel_chain_with_memory.invoke(x, config={'configurable': {'session_id': 'test_240517-1'}}))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "classify_change_or_cancel_chain_with_memory_lambda = RunnableLambda(\n",
    "    lambda x: classify_change_or_cancel_chain_with_memory.invoke(x,\n",
    "                                                                 config={\"configurable\": {\"session_id\": \"test_240517-1\"}})\n",
    ")\n",
    "classify_change_or_cancel_chain_with_memory_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a customer service assistant responsible for identifying and processing order changes based on customer chat history.\\n            First, find an AIMessage in the chat history that presents an 'order_id' and the corresponding order details, asking if the customer wishes to change the order.\\n            Then, look for the customer's response to the AIMessage regarding their desired changes. \\n            Combine the AIMessage provided order details and the customer’s input to finalize the order changes.\\n            \\n            If the customer's response indicates that the presented order details are not the ones they want to change, do not proceed with the order change as it indicates a mismatch.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001C1117B1B40>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001C1100FF400>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_change_chain = order_change_prompt | model\n",
    "order_change_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주문 변경, 주문 취소 라우트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution_route(info):\n",
    "    print(\"execution_route 함수로 전달된 데이터 -> \", info)\n",
    "    if \"주문 변경\" in info[\"action_type\"].content:\n",
    "        return \n",
    "    else:\n",
    "        return cancel_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
