{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory 공유 확인 위한 dummy chain 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "\n",
      "            You are a robot that classifies customer input messages into specific types. If order_id is provided, you need to output '조회'.\n",
      "            If order_id is not provided, you need to classify the customer input message into one of the following two types:\n",
      "            - 상품 문의, 주문 내역 조회, 주문 변경 내역 조회, 주문 취소 내역 조회: '문의'\n",
      "            - 주문 요청, 주문 변경 요청, 주문 취소 요청: '요청'\n",
      "            \n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{chat_history}\u001b[0m\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "input:\u001b[33;1m\u001b[1;3m{input}\u001b[0m\n",
      "order_id:\u001b[33;1m\u001b[1;3m{order_id}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "dummy_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types. If order_id is provided, you need to output '조회'.\n",
    "            If order_id is not provided, you need to classify the customer input message into one of the following two types:\n",
    "            - 상품 문의, 주문 내역 조회, 주문 변경 내역 조회, 주문 취소 내역 조회: '문의'\n",
    "            - 주문 요청, 주문 변경 요청, 주문 취소 요청: '요청'\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"input:{input}\\norder_id:{order_id}\"),\n",
    "    ]\n",
    ")\n",
    "dummy_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_chain = dummy_prompt | model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "dumy_chain_with_memory = RunnableWithMessageHistory(\n",
    "    dummy_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run e44217c7-7a2c-47b6-bc4e-749c1d867814 not found for run d31b156d-3079-4b03-a065-61f3f34fbdb7. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 148, 'total_tokens': 151}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-22783fa0-2a50-4280-bd25-597866a41c1e-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = dumy_chain_with_memory.invoke(\n",
    "    {\"input\": \"주문 변경하고 싶어\", \n",
    "    \"order_id\": None},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240517-3\"}}\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 변경하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 148, 'total_tokens': 151}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-22783fa0-2a50-4280-bd25-597866a41c1e-0')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240517-3\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주문 변경/취소 분류 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            You need to review the messages in the Messages Placeholder from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n\\n            Classify as '주문 변경' if there are any indications or keywords related to modifying an order, such as:\\n            - 변경\\n            - 수정\\n            - 바꾸다\\n            - 변경하고 싶어\\n\\n            Classify as '주문 취소' if there are any indications or keywords related to canceling an order, such as:\\n            - 취소\\n            - 취소하고 싶어\\n            - 취소 요청\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'order_id'], template='input:{input}\\norder_id:{order_id}'))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "order_change_cancel_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types. \n",
    "            You need to review the messages in the Messages Placeholder from the latest to the oldest and output either '주문 변경' or '주문 취소'.\n",
    "\n",
    "            Classify as '주문 변경' if there are any indications or keywords related to modifying an order, such as:\n",
    "            - 변경\n",
    "            - 수정\n",
    "            - 바꾸다\n",
    "            - 변경하고 싶어\n",
    "\n",
    "            Classify as '주문 취소' if there are any indications or keywords related to canceling an order, such as:\n",
    "            - 취소\n",
    "            - 취소하고 싶어\n",
    "            - 취소 요청\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"input:{input}\\norder_id:{order_id}\"),\n",
    "    ]\n",
    ")\n",
    "order_change_cancel_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            You need to review the messages in the Messages Placeholder from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n\\n            Classify as '주문 변경' if there are any indications or keywords related to modifying an order, such as:\\n            - 변경\\n            - 수정\\n            - 바꾸다\\n            - 변경하고 싶어\\n\\n            Classify as '주문 취소' if there are any indications or keywords related to canceling an order, such as:\\n            - 취소\\n            - 취소하고 싶어\\n            - 취소 요청\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'order_id'], template='input:{input}\\norder_id:{order_id}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002B823EC3010>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002B823EFC790>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_change_or_cancel_chain = order_change_cancel_prompt | model\n",
    "classify_change_or_cancel_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "classify_change_or_cancel_chain_with_memory = RunnableWithMessageHistory(\n",
    "    classify_change_or_cancel_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(lambda x: classify_change_or_cancel_chain_with_memory.invoke(x, config={'configurable': {'session_id': 'test_240517-3'}}))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "classify_change_or_cancel_chain_with_memory_lambda = RunnableLambda(\n",
    "    lambda x: classify_change_or_cancel_chain_with_memory.invoke(x,\n",
    "                                                                 config={\"configurable\": {\"session_id\": \"test_240517-3\"}})\n",
    ")\n",
    "classify_change_or_cancel_chain_with_memory_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 승인 메시지 식별 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['action_type', 'chat_history', 'input', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            You need to determine whether the user has responded to the AI's request for approval regarding the action specified in action_type.\\n            \\n            To make this determination, the following conditions must be met:\\n            1. There must be an AIMessage that asks for approval for the action specified in action_type, based on the specified order details.\\n            2. The user must have explicitly expressed consent in response to the AIMessage asking for approval of the action specified in action_type.\\n            \\n            Your response must be either 'yes' or 'no'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['action_type', 'input', 'order_id'], template='input:{input}\\norder_id:{order_id}\\naction_type: {action_type}'))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "classify_confirmation_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types. \n",
    "            You need to determine whether the user has responded to the AI's request for approval regarding the action specified in action_type.\n",
    "            \n",
    "            To make this determination, the following conditions must be met:\n",
    "            1. There must be an AIMessage that asks for approval for the action specified in action_type, based on the specified order details.\n",
    "            2. The user must have explicitly expressed consent in response to the AIMessage asking for approval of the action specified in action_type.\n",
    "            \n",
    "            Your response must be either 'yes' or 'no'.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"input:{input}\\norder_id:{order_id}\\naction_type: {action_type}\"),\n",
    "    ]\n",
    ")\n",
    "classify_confirmation_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['action_type', 'chat_history', 'input', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            You need to determine whether the user has responded to the AI's request for approval regarding the action specified in action_type.\\n            \\n            To make this determination, the following conditions must be met:\\n            1. There must be an AIMessage that asks for approval for the action specified in action_type, based on the specified order details.\\n            2. The user must have explicitly expressed consent in response to the AIMessage asking for approval of the action specified in action_type.\\n            \\n            Your response must be either 'yes' or 'no'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['action_type', 'input', 'order_id'], template='input:{input}\\norder_id:{order_id}\\naction_type: {action_type}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002B823EC3010>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002B823EFC790>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_confirmation_chain = classify_confirmation_prompt | model \n",
    "classify_confirmation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "classify_confirmation_chain_with_memory = RunnableWithMessageHistory(\n",
    "    classify_confirmation_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(lambda x: classify_confirmation_chain_with_memory.invoke(x, config={'configurable': {'session_id': 'test_240517-3'}}))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "classify_confirmation_chain_with_memory_lambda = RunnableLambda(\n",
    "    lambda x: classify_confirmation_chain_with_memory.invoke(x,\n",
    "                                                             config={\"configurable\": {\"session_id\": \"test_240517-3\"}})\n",
    ")\n",
    "classify_confirmation_chain_with_memory_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## action_type 추가 위한 보조 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(dict):\n",
    "    inputs = dict[\"inputs\"]\n",
    "    action_type =  dict[\"action_type\"]\n",
    "    # action_type = action_type.content\n",
    "    inputs[\"action_type\"] = action_type\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 종합-입력에 승인 메시지 포함 여부까지 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  inputs: RunnablePassthrough(),\n",
       "  action_type: RunnableLambda(lambda x: classify_change_or_cancel_chain_with_memory.invoke(x, config={'configurable': {'session_id': 'test_240517-3'}}))\n",
       "}\n",
       "| RunnableLambda(helper)\n",
       "| RunnableAssign(mapper={\n",
       "    execution_confirmation: RunnableLambda(lambda x: classify_confirmation_chain_with_memory.invoke(x, config={'configurable': {'session_id': 'test_240517-3'}}))\n",
       "  })"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "confirmation_chain = (\n",
    "    {\n",
    "        \"inputs\": RunnablePassthrough(),\n",
    "        \"action_type\": classify_change_or_cancel_chain_with_memory_lambda\n",
    "    } \n",
    "    | RunnableLambda(helper) \n",
    "    | RunnablePassthrough.assign(execution_confirmation=classify_confirmation_chain_with_memory_lambda)\n",
    "    )\n",
    "confirmation_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 승인 메시지 작성 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['action_type', 'queried_result'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\n            You are a robot that generates a confirmation request message based on the action_type and queried_result.\\n            create a message to show the queried_result and ask for final confirmation considering action_type.\\n            The response should be generated in Korean.\\n            ')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['action_type', 'queried_result'], template='action_type:{action_type}\\nqueried_result:{queried_result}'))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "generate_confirm_message_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that generates a confirmation request message based on the action_type and queried_result.\n",
    "            create a message to show the queried_result and ask for final confirmation considering action_type.\n",
    "            The response should be generated in Korean.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        # MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"action_type:{action_type}\\nqueried_result:{queried_result}\")\n",
    "    ]\n",
    ")\n",
    "generate_confirm_message_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['action_type', 'queried_result'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\n            You are a robot that generates a confirmation request message based on the action_type and queried_result.\\n            create a message to show the queried_result and ask for final confirmation considering action_type.\\n            The response should be generated in Korean.\\n            ')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['action_type', 'queried_result'], template='action_type:{action_type}\\nqueried_result:{queried_result}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002B823EC3010>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002B823EFC790>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_confirm_message_chain = generate_confirm_message_prompt | model\n",
    "generate_confirm_message_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "generate_confirm_message_chain_with_memory = RunnableWithMessageHistory(\n",
    "    generate_confirm_message_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(lambda x: generate_confirm_message_chain_with_memory.invoke(x, config={'configurable': {'session_id': 'test_240517-3'}}))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "generate_confirm_message_chain_with_memory_lambda = RunnableLambda(\n",
    "    lambda x: generate_confirm_message_chain_with_memory.invoke(x,\n",
    "                                                             config={\"configurable\": {\"session_id\": \"test_240517-3\"}})\n",
    ")\n",
    "generate_confirm_message_chain_with_memory_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주문 조회 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_recent_orders(dict):\n",
    "    print(dict)\n",
    "    return \"produdct: 백설기 quantity: 2 price: 13,000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 주문 변경 처리 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "order_change_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a customer service assistant responsible for identifying and processing order changes based on customer chat history.\n",
    "            First, find an AIMessage in the chat history that presents an 'order_id' and the corresponding order details, asking if the customer wishes to change the order.\n",
    "            Then, look for the customer's response to the AIMessage regarding their desired changes. \n",
    "            Combine the AIMessage provided order details and the customer’s input to finalize the order changes.\n",
    "            \n",
    "            If the customer's response indicates that the presented order details are not the ones they want to change, do not proceed with the order change as it indicates a mismatch.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        # MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a customer service assistant responsible for identifying and processing order changes based on customer chat history.\\n            First, find an AIMessage in the chat history that presents an 'order_id' and the corresponding order details, asking if the customer wishes to change the order.\\n            Then, look for the customer's response to the AIMessage regarding their desired changes. \\n            Combine the AIMessage provided order details and the customer’s input to finalize the order changes.\\n            \\n            If the customer's response indicates that the presented order details are not the ones they want to change, do not proceed with the order change as it indicates a mismatch.\\n            \")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002B823EC3010>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002B823EFC790>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_change_chain = order_change_prompt | model\n",
    "order_change_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "order_change_chain_with_memory = RunnableWithMessageHistory(\n",
    "    order_change_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_change_chain_with_memory_lambda = RunnableLambda(\n",
    "    lambda x: order_change_chain_with_memory.invoke(x,\n",
    "                                                    config={\"configurable\": {\"session_id\": \"test_240517-3\"}})\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 승인 메시지 포함 여부에 따른 라우트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution_or_message_route(info):\n",
    "    print(\"execution_or_message_route 함수로 전달된 데이터 -> \", info)\n",
    "    if \"yes\" in info[\"execution_confirmation\"].content.lower():\n",
    "        return order_change_chain_with_memory_lambda\n",
    "    else:\n",
    "        return RunnablePassthrough.assign(queried_result=fetch_recent_orders) | generate_confirm_message_chain_with_memory_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  inputs: RunnablePassthrough(),\n",
       "  action_type: RunnableLambda(lambda x: classify_change_or_cancel_chain_with_memory.invoke(x, config={'configurable': {'session_id': 'test_240517-3'}}))\n",
       "}\n",
       "| RunnableLambda(helper)\n",
       "| RunnableAssign(mapper={\n",
       "    execution_confirmation: RunnableLambda(lambda x: classify_confirmation_chain_with_memory.invoke(x, config={'configurable': {'session_id': 'test_240517-3'}}))\n",
       "  })\n",
       "| RunnableLambda(execution_or_message_route)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution_chain = confirmation_chain | execution_or_message_route\n",
    "execution_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 80a088b3-2460-40b9-82c9-448a83925449 not found for run 0b18b5e9-402f-4ef9-aead-003b84a40b03. Treating as a root run.\n",
      "Parent run 452f7090-12c7-4747-b8ba-7fbb0a0ba883 not found for run 18c4317d-a1c0-49e7-98d6-917656b1ed1c. Treating as a root run.\n",
      "Parent run fafef46f-83e6-4b59-b838-f655524815cd not found for run 210ecf68-28b3-412d-b85e-2c4c67e988a6. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution_or_message_route 함수로 전달된 데이터 ->  {'input': '없음', 'order_id': 7, 'action_type': AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 195, 'total_tokens': 198}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3ede3143-e0cd-4e95-aa7e-98eec2a2ae50-0'), 'execution_confirmation': AIMessage(content='no', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 285, 'total_tokens': 286}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b2121e9d-45c7-47db-ac2f-ce8a9796806d-0')}\n",
      "{'input': '없음', 'order_id': 7, 'action_type': AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 195, 'total_tokens': 198}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3ede3143-e0cd-4e95-aa7e-98eec2a2ae50-0'), 'execution_confirmation': AIMessage(content='no', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 285, 'total_tokens': 286}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b2121e9d-45c7-47db-ac2f-ce8a9796806d-0')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='주문 내용을 변경하려고 합니다.\\n\\n상품: 백설기\\n수량: 2개\\n가격: 13,000원\\n\\n변경 사항을 최종 확인해 주세요. 변경하시겠습니까?', response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 185, 'total_tokens': 253}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0dd09085-5b05-4aa4-96e8-a7ca10b0c0ae-0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = execution_chain.invoke(\n",
    "    {\"input\": '없음',\n",
    "     \"order_id\": 7}\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 변경하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 148, 'total_tokens': 151}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-22783fa0-2a50-4280-bd25-597866a41c1e-0'),\n",
       " HumanMessage(content='없음'),\n",
       " AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 195, 'total_tokens': 198}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3ede3143-e0cd-4e95-aa7e-98eec2a2ae50-0'),\n",
       " HumanMessage(content='없음'),\n",
       " AIMessage(content='no', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 285, 'total_tokens': 286}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b2121e9d-45c7-47db-ac2f-ce8a9796806d-0'),\n",
       " HumanMessage(content='없음'),\n",
       " AIMessage(content='주문 내용을 변경하려고 합니다.\\n\\n상품: 백설기\\n수량: 2개\\n가격: 13,000원\\n\\n변경 사항을 최종 확인해 주세요. 변경하시겠습니까?', response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 185, 'total_tokens': 253}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0dd09085-5b05-4aa4-96e8-a7ca10b0c0ae-0')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240517-3\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 84fb21c8-b732-40e0-bed3-fecbd730b82d not found for run 637b7fb3-fd9a-499d-80b0-78e81141b19a. Treating as a root run.\n",
      "Parent run 5a864ea0-7f5c-4d26-81a0-9271a8e8d78d not found for run 961ccf01-e051-4482-ab6b-98f0b8e0fd93. Treating as a root run.\n",
      "Parent run 7e9f6317-84ee-4ab7-9476-d9ee7ac7af35 not found for run 11c14bbb-744c-4471-890c-5285dbc620c5. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution_or_message_route 함수로 전달된 데이터 ->  {'input': '떡케익 13호로 바꿀게요', 'order_id': 7, 'action_type': AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 315, 'total_tokens': 318}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f78aaf21-af09-4141-986a-64328a3c2150-0'), 'execution_confirmation': AIMessage(content='no', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 418, 'total_tokens': 419}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-adddd686-4595-443e-a69e-ef341edb0974-0')}\n",
      "{'input': '떡케익 13호로 바꿀게요', 'order_id': 7, 'action_type': AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 315, 'total_tokens': 318}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f78aaf21-af09-4141-986a-64328a3c2150-0'), 'execution_confirmation': AIMessage(content='no', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 418, 'total_tokens': 419}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-adddd686-4595-443e-a69e-ef341edb0974-0')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='주문을 변경하려고 합니다. 다음 내용으로 주문을 변경하시겠습니까?\\n\\n상품: 백설기\\n수량: 2개\\n가격: 13,000원\\n\\n최종 확정을 부탁드립니다.', response_metadata={'token_usage': {'completion_tokens': 75, 'prompt_tokens': 183, 'total_tokens': 258}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ff68b147-6d15-49e3-8649-75902bed8da5-0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = execution_chain.invoke(\n",
    "    {\"input\": '떡케익 13호로 바꿀게요',\n",
    "     \"order_id\": 7}\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 변경하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 148, 'total_tokens': 151}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-22783fa0-2a50-4280-bd25-597866a41c1e-0'),\n",
       " HumanMessage(content='없음'),\n",
       " AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 195, 'total_tokens': 198}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3ede3143-e0cd-4e95-aa7e-98eec2a2ae50-0'),\n",
       " HumanMessage(content='없음'),\n",
       " AIMessage(content='no', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 285, 'total_tokens': 286}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b2121e9d-45c7-47db-ac2f-ce8a9796806d-0'),\n",
       " HumanMessage(content='없음'),\n",
       " AIMessage(content='주문 내용을 변경하려고 합니다.\\n\\n상품: 백설기\\n수량: 2개\\n가격: 13,000원\\n\\n변경 사항을 최종 확인해 주세요. 변경하시겠습니까?', response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 185, 'total_tokens': 253}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0dd09085-5b05-4aa4-96e8-a7ca10b0c0ae-0'),\n",
       " HumanMessage(content='떡케익 13호로 바꿀게요'),\n",
       " AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 315, 'total_tokens': 318}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f78aaf21-af09-4141-986a-64328a3c2150-0'),\n",
       " HumanMessage(content='떡케익 13호로 바꿀게요'),\n",
       " AIMessage(content='no', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 418, 'total_tokens': 419}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-adddd686-4595-443e-a69e-ef341edb0974-0'),\n",
       " HumanMessage(content='떡케익 13호로 바꿀게요'),\n",
       " AIMessage(content='주문을 변경하려고 합니다. 다음 내용으로 주문을 변경하시겠습니까?\\n\\n상품: 백설기\\n수량: 2개\\n가격: 13,000원\\n\\n최종 확정을 부탁드립니다.', response_metadata={'token_usage': {'completion_tokens': 75, 'prompt_tokens': 183, 'total_tokens': 258}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ff68b147-6d15-49e3-8649-75902bed8da5-0')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240517-3\"].messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
