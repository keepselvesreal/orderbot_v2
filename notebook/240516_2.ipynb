{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory 공유 확인 위한 dummy chain 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "dummy_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types. If order_id is provided, you need to output '조회'.\n",
    "            If order_id is not provided, you need to classify the customer input message into one of the following two types:\n",
    "            - 상품 문의, 주문 내역 조회, 주문 변경 내역 조회, 주문 취소 내역 조회: '문의'\n",
    "            - 주문 요청, 주문 변경 요청, 주문 취소 요청: '요청'\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"input:{input}\\norder_id:{order_id}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_chain = dummy_prompt | model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "dumy_chain_with_memory = RunnableWithMessageHistory(\n",
    "    dummy_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run fe98663a-199a-4811-97e7-32eb1dc9b7c9 not found for run 272b1ec9-d7ab-4f7b-923d-ee09c5b010c1. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 150, 'total_tokens': 153}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ddadc334-cda2-4ae0-83bf-40cc0b23938a-0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = dumy_chain_with_memory.invoke(\n",
    "    {\"input\": \"주문 취소하고 싶어\", \n",
    "    \"order_id\": None},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = dumy_chain_with_memory.invoke(\n",
    "    {\"input\": \"주문 취소하고 싶어\", \n",
    "    \"order_id\": None},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 27f97e00-2bd8-437c-9c98-a9b0f1f31573 not found for run 414bb4aa-89e4-45b1-b563-12c27c8c9bc2. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 169, 'total_tokens': 172}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0b352454-e795-485a-a26a-c38bada679c7-0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = dumy_chain_with_memory.invoke(\n",
    "    {\"input\": \"주문 변경하고 싶어\", \n",
    "    \"order_id\": None},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험 체인 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "order_change_cancel_prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types. \n",
    "            you need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"order_id:{order_id}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_chain = order_change_cancel_prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "classify_chain_with_memory = RunnableWithMessageHistory(\n",
    "    classify_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"order_id\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 69269b3c-174f-49f6-aefd-d949896c37cd not found for run dde42eb9-d775-49bf-a2e0-76fa1eedffef. Treating as a root run.\n",
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='주문 취소', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 83, 'total_tokens': 88}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-79530680-f4ca-4ccf-9208-2908dd63632c-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = classify_chain_with_memory.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 취소하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 150, 'total_tokens': 153}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ddadc334-cda2-4ae0-83bf-40cc0b23938a-0')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240516-1\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 2eb3a18c-91d1-4301-8ae2-b6c28f6083c5 not found for run 00408ca8-c8f7-4591-9a8e-926bf6581083. Treating as a root run.\n",
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 102, 'total_tokens': 105}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ec19f61f-1e55-46c6-88f5-6b057bfe8f6c-0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = classify_chain_with_memory.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classify_chain_with_memory의 출력은 저장이 안 되네?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 취소하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 150, 'total_tokens': 153}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ddadc334-cda2-4ae0-83bf-40cc0b23938a-0'),\n",
       " HumanMessage(content='주문 변경하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 169, 'total_tokens': 172}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0b352454-e795-485a-a26a-c38bada679c7-0')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240516-1\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## order_id 주어졌을 때 처리 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run a5270db7-107f-4bab-a48c-3506bc7e2053 not found for run 9262573e-ab0b-4e2f-bd27-3d2c314a8eed. Treating as a root run.\n",
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    }
   ],
   "source": [
    "response = classify_chain_with_memory.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_recent_orders(dict):\n",
    "    print(dict)\n",
    "    return \"produdct: 백설기 quantity: 2 price: 13,000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "generate_confirm_message_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that generates a confirmation request message based on the check_purpose and queried_result.\n",
    "            create a message to show the queried_result and ask for final confirmation considering check_purpose.\n",
    "            The response should be generated in Korean.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        # MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"check_purpose:{check_purpose}\\nqueried_result:{queried_result}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confirm_message_chain = generate_confirm_message_prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  order_id: RunnablePassthrough(),\n",
       "  check_purpose: RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "                   chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "                 }), config={'run_name': 'insert_history'})\n",
       "                 | RunnableBinding(bound=ChatPromptTemplate(input_variables=['chat_history', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            you need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['order_id'], template='order_id:{order_id}'))])\n",
       "                   | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E6568610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E6568BB0>, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x00000235E655FA30>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x00000235C5AAB5B0>, input_messages_key='order_id', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)]),\n",
       "  queried_result: RunnableLambda(fetch_recent_orders)\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['check_purpose', 'queried_result'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\n            You are a robot that generates a confirmation request message based on the check_purpose and queried_result.\\n            create a message to show the queried_result and ask for final confirmation considering check_purpose.\\n            The response should be generated in Korean.\\n            ')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['check_purpose', 'queried_result'], template='check_purpose:{check_purpose}\\nqueried_result:{queried_result}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E6568610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E6568BB0>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "test_chain1 = {\n",
    "    \"order_id\": RunnablePassthrough(),\n",
    "    \"check_purpose\": classify_chain_with_memory,\n",
    "    \"queried_result\": RunnableLambda(fetch_recent_orders)\n",
    "} | generate_confirm_message_chain\n",
    "test_chain1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 취소하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 150, 'total_tokens': 153}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ddadc334-cda2-4ae0-83bf-40cc0b23938a-0'),\n",
       " HumanMessage(content='주문 변경하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 169, 'total_tokens': 172}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0b352454-e795-485a-a26a-c38bada679c7-0')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240516-1\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 28438fb1-5617-465c-84f5-46f8bb2e1502 not found for run 4280a95f-4268-45d5-be10-30dff4d1c810. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'order_id': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='주문 내용을 확인해보실래요?\\n\\n상품: 백설기\\n수량: 2개\\n가격: 13,000원\\n\\n주문 변경을 원하시나요? 최종 확인 부탁드립니다.', response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 185, 'total_tokens': 251}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-215495d7-2c89-4565-8cdd-cdb7b4a3bbf0-0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_chain1.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(dict):\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  order_id: RunnablePassthrough(),\n",
       "  check_purpose: RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "                   chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "                 }), config={'run_name': 'insert_history'})\n",
       "                 | RunnableBinding(bound=ChatPromptTemplate(input_variables=['chat_history', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            you need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['order_id'], template='order_id:{order_id}'))])\n",
       "                   | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E6568610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E6568BB0>, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x00000235E655FA30>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x00000235C5AAB5B0>, input_messages_key='order_id', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)]),\n",
       "  queried_result: RunnableLambda(fetch_recent_orders)\n",
       "}\n",
       "| RunnableLambda(debug)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "try_fix_chain = {\n",
    "    \"order_id\": RunnablePassthrough(),\n",
    "    \"check_purpose\": classify_chain_with_memory,\n",
    "    \"queried_result\": RunnableLambda(fetch_recent_orders)\n",
    "} | RunnableLambda(debug)\n",
    "try_fix_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 47b42aa4-32f2-4813-8301-ba207c062577 not found for run c8fac5b0-f8c0-4253-8c93-9cfe33b3900b. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'order_id': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'order_id': {'order_id': 3},\n",
       " 'check_purpose': AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 102, 'total_tokens': 105}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4c373960-6f10-4750-b92c-a178700cf3d1-0'),\n",
       " 'queried_result': 'produdct: 백설기 quantity: 2 price: 13,000'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_fix_chain.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "str_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  order_id: RunnablePassthrough(),\n",
       "  check_purpose: RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "                   chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "                 }), config={'run_name': 'insert_history'})\n",
       "                 | RunnableBinding(bound=ChatPromptTemplate(input_variables=['chat_history', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            you need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['order_id'], template='order_id:{order_id}'))])\n",
       "                   | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E6568610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E6568BB0>, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x00000235E655FA30>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x00000235C5AAB5B0>, input_messages_key='order_id', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])\n",
       "                 | StrOutputParser(),\n",
       "  queried_result: RunnableLambda(fetch_recent_orders)\n",
       "}\n",
       "| RunnableLambda(debug)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "try_fix_chain = {\n",
    "    \"order_id\": RunnablePassthrough(),\n",
    "    \"check_purpose\": classify_chain_with_memory | str_parser,\n",
    "    \"queried_result\": RunnableLambda(fetch_recent_orders)\n",
    "} | RunnableLambda(debug)\n",
    "try_fix_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 1a4e0759-d493-482a-b938-614b5d870405 not found for run b503f226-03e9-4427-8cb0-c5268147188f. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'order_id': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'order_id': {'order_id': 3},\n",
       " 'check_purpose': '주문 변경',\n",
       " 'queried_result': 'produdct: 백설기 quantity: 2 price: 13,000'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_fix_chain.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  order_id: RunnableLambda(...),\n",
       "  check_purpose: RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "                   chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "                 }), config={'run_name': 'insert_history'})\n",
       "                 | RunnableBinding(bound=ChatPromptTemplate(input_variables=['chat_history', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            you need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['order_id'], template='order_id:{order_id}'))])\n",
       "                   | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E6568610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E6568BB0>, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x00000235E655FA30>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x00000235C5AAB5B0>, input_messages_key='order_id', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])\n",
       "                 | StrOutputParser(),\n",
       "  queried_result: RunnableLambda(fetch_recent_orders)\n",
       "}\n",
       "| RunnableLambda(debug)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "try_fix_chain = {\n",
    "    \"order_id\": lambda input: input[\"order_id\"],\n",
    "    \"check_purpose\": classify_chain_with_memory | str_parser,\n",
    "    \"queried_result\": RunnableLambda(fetch_recent_orders)\n",
    "} | RunnableLambda(debug)\n",
    "try_fix_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 0e9721e1-48b9-4369-9a0b-439b8f126cde not found for run 5af93389-9543-4111-b90c-d1f3b21dee76. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'order_id': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'order_id': 3,\n",
       " 'check_purpose': '주문 변경',\n",
       " 'queried_result': 'produdct: 백설기 quantity: 2 price: 13,000'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_fix_chain.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "test_chain1 = {\n",
    "    \"order_id\": RunnablePassthrough(),\n",
    "    \"check_purpose\": classify_chain_with_memory,\n",
    "    \"queried_result\": RunnableLambda(fetch_recent_orders)\n",
    "} | generate_confirm_message_chain\n",
    "test_chain1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "}), config={'run_name': 'insert_history'})\n",
       "| RunnableBinding(bound=ChatPromptTemplate(input_variables=['chat_history', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            you need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['order_id'], template='order_id:{order_id}'))])\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E6568610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E6568BB0>, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x00000235E655FA30>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x00000235C5AAB5B0>, input_messages_key='order_id', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])\n",
       "| RunnableAssign(mapper={\n",
       "    check_purpose: RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "                     chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "                   }), config={'run_name': 'insert_history'})\n",
       "                   | RunnableBinding(bound=ChatPromptTemplate(input_variables=['chat_history', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            you need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['order_id'], template='order_id:{order_id}'))])\n",
       "                     | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E6568610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E6568BB0>, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x00000235E655FA30>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x00000235C5AAB5B0>, input_messages_key='order_id', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])\n",
       "  })\n",
       "| RunnableAssign(mapper={\n",
       "    queried_result: RunnableLambda(fetch_recent_orders)\n",
       "  })\n",
       "| ChatPromptTemplate(input_variables=['check_purpose', 'queried_result'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\n            You are a robot that generates a confirmation request message based on the check_purpose and queried_result.\\n            create a message to show the queried_result and ask for final confirmation considering check_purpose.\\n            The response should be generated in Korean.\\n            ')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['check_purpose', 'queried_result'], template='check_purpose:{check_purpose}\\nqueried_result:{queried_result}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E6568610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E6568BB0>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_chain2 = (\n",
    "    classify_chain_with_memory | \n",
    "    RunnablePassthrough.assign(check_purpose=classify_chain_with_memory) | \n",
    "    RunnablePassthrough.assign(queried_result=RunnableLambda(fetch_recent_orders)) |\n",
    "    generate_confirm_message_chain\n",
    "    )\n",
    "test_chain2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 94f73d1e-cd00-49dd-bce4-fb2967cf08bc not found for run 3bbec3cf-a86f-4ed7-bdc7-ba980409a60e. Treating as a root run.\n",
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The input to RunnablePassthrough.assign() must be a dict.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_chain2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morder_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigurable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msession_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_240516-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py:469\u001b[0m, in \u001b[0;36mRunnableAssign.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    466\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    468\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:1626\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1623\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[0;32m   1624\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1625\u001b[0m         Output,\n\u001b[1;32m-> 1626\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1627\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1629\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1630\u001b[0m             config,\n\u001b[0;32m   1631\u001b[0m             run_manager,\n\u001b[0;32m   1632\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1633\u001b[0m         ),\n\u001b[0;32m   1634\u001b[0m     )\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1636\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py:450\u001b[0m, in \u001b[0;36mRunnableAssign._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke\u001b[39m(\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    449\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 450\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m\n\u001b[0;32m    452\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    456\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapper\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    460\u001b[0m         ),\n\u001b[0;32m    461\u001b[0m     }\n",
      "\u001b[1;31mAssertionError\u001b[0m: The input to RunnablePassthrough.assign() must be a dict."
     ]
    }
   ],
   "source": [
    "test_chain2.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 확인 요청 메시지 작성 이후 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 28438fb1-5617-465c-84f5-46f8bb2e1502 not found for run 4280a95f-4268-45d5-be10-30dff4d1c810. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'order_id': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='주문 내용을 확인해보실래요?\\n\\n상품: 백설기\\n수량: 2개\\n가격: 13,000원\\n\\n주문 변경을 원하시나요? 최종 확인 부탁드립니다.', response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 185, 'total_tokens': 251}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-215495d7-2c89-4565-8cdd-cdb7b4a3bbf0-0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_chain1.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
