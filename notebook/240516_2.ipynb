{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory 공유 확인 위한 dummy chain 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "dummy_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types. If order_id is provided, you need to output '조회'.\n",
    "            If order_id is not provided, you need to classify the customer input message into one of the following two types:\n",
    "            - 상품 문의, 주문 내역 조회, 주문 변경 내역 조회, 주문 취소 내역 조회: '문의'\n",
    "            - 주문 요청, 주문 변경 요청, 주문 취소 요청: '요청'\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"input:{input}\\norder_id:{order_id}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_chain = dummy_prompt | model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "dumy_chain_with_memory = RunnableWithMessageHistory(\n",
    "    dummy_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 5b6f3689-b45b-445b-965a-94ac14dcffe0 not found for run dcde2ba9-4a28-4e19-abb6-cf3ff4a30c6d. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 150, 'total_tokens': 153}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-fb8eaa24-821d-4fb6-acf7-ea01e3b85ad5-0')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = dumy_chain_with_memory.invoke(\n",
    "    {\"input\": \"주문 취소하고 싶어\", \n",
    "    \"order_id\": None},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 0ec58752-0062-4b05-a342-c4256e369cef not found for run c22ad19f-71f3-434d-af05-51696c8358fb. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 171, 'total_tokens': 174}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c77152bb-847c-49bc-9bb3-8658bd70fe1c-0')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = dumy_chain_with_memory.invoke(\n",
    "    {\"input\": \"주문 취소하고 싶어\", \n",
    "    \"order_id\": None},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run a2a143d4-d597-446d-afa2-c12ef92c36cc not found for run 18fb22c3-cb24-4922-be01-afa48e74c540. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='문의', response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 190, 'total_tokens': 192}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e06e6ec6-d64d-480a-ab79-b793f847de0f-0')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = dumy_chain_with_memory.invoke(\n",
    "    {\"input\": \"주문 변경하고 싶어\", \n",
    "    \"order_id\": None},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험 체인 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "order_change_cancel_prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types. \n",
    "            you need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"order_id:{order_id}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_chain = order_change_cancel_prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "classify_chain_with_memory = RunnableWithMessageHistory(\n",
    "    classify_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"order_id\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 69269b3c-174f-49f6-aefd-d949896c37cd not found for run dde42eb9-d775-49bf-a2e0-76fa1eedffef. Treating as a root run.\n",
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='주문 취소', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 83, 'total_tokens': 88}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-79530680-f4ca-4ccf-9208-2908dd63632c-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = classify_chain_with_memory.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 취소하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 150, 'total_tokens': 153}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ddadc334-cda2-4ae0-83bf-40cc0b23938a-0')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240516-1\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 2eb3a18c-91d1-4301-8ae2-b6c28f6083c5 not found for run 00408ca8-c8f7-4591-9a8e-926bf6581083. Treating as a root run.\n",
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 102, 'total_tokens': 105}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ec19f61f-1e55-46c6-88f5-6b057bfe8f6c-0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = classify_chain_with_memory.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classify_chain_with_memory의 출력은 저장이 안 되네?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 취소하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 150, 'total_tokens': 153}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ddadc334-cda2-4ae0-83bf-40cc0b23938a-0'),\n",
       " HumanMessage(content='주문 변경하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 169, 'total_tokens': 172}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0b352454-e795-485a-a26a-c38bada679c7-0')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240516-1\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## order_id 주어졌을 때 처리 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run a5270db7-107f-4bab-a48c-3506bc7e2053 not found for run 9262573e-ab0b-4e2f-bd27-3d2c314a8eed. Treating as a root run.\n",
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    }
   ],
   "source": [
    "response = classify_chain_with_memory.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_recent_orders(dict):\n",
    "    print(dict)\n",
    "    return \"produdct: 백설기 quantity: 2 price: 13,000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "generate_confirm_message_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that generates a confirmation request message based on the check_purpose and queried_result.\n",
    "            create a message to show the queried_result and ask for final confirmation considering check_purpose.\n",
    "            The response should be generated in Korean.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        # MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"check_purpose:{check_purpose}\\nqueried_result:{queried_result}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_confirm_message_chain = generate_confirm_message_prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  order_id: RunnablePassthrough(),\n",
       "  check_purpose: RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "                   chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "                 }), config={'run_name': 'insert_history'})\n",
       "                 | RunnableBinding(bound=ChatPromptTemplate(input_variables=['chat_history', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            you need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['order_id'], template='order_id:{order_id}'))])\n",
       "                   | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E6568610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E6568BB0>, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x00000235E655FA30>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x00000235C5AAB5B0>, input_messages_key='order_id', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)]),\n",
       "  queried_result: RunnableLambda(fetch_recent_orders)\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['check_purpose', 'queried_result'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\n            You are a robot that generates a confirmation request message based on the check_purpose and queried_result.\\n            create a message to show the queried_result and ask for final confirmation considering check_purpose.\\n            The response should be generated in Korean.\\n            ')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['check_purpose', 'queried_result'], template='check_purpose:{check_purpose}\\nqueried_result:{queried_result}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E6568610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E6568BB0>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "test_chain1 = {\n",
    "    \"order_id\": RunnablePassthrough(),\n",
    "    \"check_purpose\": classify_chain_with_memory,\n",
    "    \"queried_result\": RunnableLambda(fetch_recent_orders)\n",
    "} | generate_confirm_message_chain\n",
    "test_chain1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 취소하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 150, 'total_tokens': 153}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-ddadc334-cda2-4ae0-83bf-40cc0b23938a-0'),\n",
       " HumanMessage(content='주문 변경하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 169, 'total_tokens': 172}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0b352454-e795-485a-a26a-c38bada679c7-0')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240516-1\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 28438fb1-5617-465c-84f5-46f8bb2e1502 not found for run 4280a95f-4268-45d5-be10-30dff4d1c810. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'order_id': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='주문 내용을 확인해보실래요?\\n\\n상품: 백설기\\n수량: 2개\\n가격: 13,000원\\n\\n주문 변경을 원하시나요? 최종 확인 부탁드립니다.', response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 185, 'total_tokens': 251}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-215495d7-2c89-4565-8cdd-cdb7b4a3bbf0-0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_chain1.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(dict):\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  order_id: RunnablePassthrough(),\n",
       "  check_purpose: RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "                   chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "                 }), config={'run_name': 'insert_history'})\n",
       "                 | RunnableBinding(bound=ChatPromptTemplate(input_variables=['chat_history', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            you need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['order_id'], template='order_id:{order_id}'))])\n",
       "                   | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E6568610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E6568BB0>, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x00000235E655FA30>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x00000235C5AAB5B0>, input_messages_key='order_id', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)]),\n",
       "  queried_result: RunnableLambda(fetch_recent_orders)\n",
       "}\n",
       "| RunnableLambda(debug)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "try_fix_chain = {\n",
    "    \"order_id\": RunnablePassthrough(),\n",
    "    \"check_purpose\": classify_chain_with_memory,\n",
    "    \"queried_result\": RunnableLambda(fetch_recent_orders)\n",
    "} | RunnableLambda(debug)\n",
    "try_fix_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 47b42aa4-32f2-4813-8301-ba207c062577 not found for run c8fac5b0-f8c0-4253-8c93-9cfe33b3900b. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'order_id': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'order_id': {'order_id': 3},\n",
       " 'check_purpose': AIMessage(content='주문 변경', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 102, 'total_tokens': 105}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4c373960-6f10-4750-b92c-a178700cf3d1-0'),\n",
       " 'queried_result': 'produdct: 백설기 quantity: 2 price: 13,000'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_fix_chain.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "str_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  order_id: RunnablePassthrough(),\n",
       "  check_purpose: RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "                   chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "                 }), config={'run_name': 'insert_history'})\n",
       "                 | RunnableBinding(bound=ChatPromptTemplate(input_variables=['chat_history', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            you need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['order_id'], template='order_id:{order_id}'))])\n",
       "                   | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E6568610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E6568BB0>, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x00000235E655FA30>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x00000235C5AAB5B0>, input_messages_key='order_id', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])\n",
       "                 | StrOutputParser(),\n",
       "  queried_result: RunnableLambda(fetch_recent_orders)\n",
       "}\n",
       "| RunnableLambda(debug)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "try_fix_chain = {\n",
    "    \"order_id\": RunnablePassthrough(),\n",
    "    \"check_purpose\": classify_chain_with_memory | str_parser,\n",
    "    \"queried_result\": RunnableLambda(fetch_recent_orders)\n",
    "} | RunnableLambda(debug)\n",
    "try_fix_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 1a4e0759-d493-482a-b938-614b5d870405 not found for run b503f226-03e9-4427-8cb0-c5268147188f. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'order_id': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'order_id': {'order_id': 3},\n",
       " 'check_purpose': '주문 변경',\n",
       " 'queried_result': 'produdct: 백설기 quantity: 2 price: 13,000'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_fix_chain.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  order_id: RunnableLambda(...),\n",
       "  check_purpose: RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "                   chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "                 }), config={'run_name': 'insert_history'})\n",
       "                 | RunnableBinding(bound=ChatPromptTemplate(input_variables=['chat_history', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            you need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['order_id'], template='order_id:{order_id}'))])\n",
       "                   | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E6568610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E6568BB0>, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x00000235E655FA30>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x00000235C5AAB5B0>, input_messages_key='order_id', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])\n",
       "                 | StrOutputParser(),\n",
       "  queried_result: RunnableLambda(fetch_recent_orders)\n",
       "}\n",
       "| RunnableLambda(debug)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "try_fix_chain = {\n",
    "    \"order_id\": lambda input: input[\"order_id\"],\n",
    "    \"check_purpose\": classify_chain_with_memory | str_parser,\n",
    "    \"queried_result\": RunnableLambda(fetch_recent_orders)\n",
    "} | RunnableLambda(debug)\n",
    "try_fix_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 0e9721e1-48b9-4369-9a0b-439b8f126cde not found for run 5af93389-9543-4111-b90c-d1f3b21dee76. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'order_id': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'order_id': 3,\n",
       " 'check_purpose': '주문 변경',\n",
       " 'queried_result': 'produdct: 백설기 quantity: 2 price: 13,000'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_fix_chain.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "test_chain1 = {\n",
    "    \"order_id\": RunnablePassthrough(),\n",
    "    \"check_purpose\": classify_chain_with_memory,\n",
    "    \"queried_result\": RunnableLambda(fetch_recent_orders)\n",
    "} | generate_confirm_message_chain\n",
    "test_chain1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "}), config={'run_name': 'insert_history'})\n",
       "| RunnableBinding(bound=ChatPromptTemplate(input_variables=['chat_history', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            you need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['order_id'], template='order_id:{order_id}'))])\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E6568610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E6568BB0>, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x00000235E655FA30>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x00000235C5AAB5B0>, input_messages_key='order_id', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])\n",
       "| RunnableAssign(mapper={\n",
       "    check_purpose: RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "                     chat_history: RunnableBinding(bound=RunnableLambda(_enter_history), config={'run_name': 'load_history'})\n",
       "                   }), config={'run_name': 'insert_history'})\n",
       "                   | RunnableBinding(bound=ChatPromptTemplate(input_variables=['chat_history', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\n            You are a robot that classifies customer input messages into specific types. \\n            you need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n            \")), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['order_id'], template='order_id:{order_id}'))])\n",
       "                     | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E6568610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E6568BB0>, openai_api_key=SecretStr('**********'), openai_proxy=''), config_factories=[<function Runnable.with_listeners.<locals>.<lambda> at 0x00000235E655FA30>]), config={'run_name': 'RunnableWithMessageHistory'}), get_session_history=<function get_session_history at 0x00000235C5AAB5B0>, input_messages_key='order_id', history_messages_key='chat_history', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])\n",
       "  })\n",
       "| RunnableAssign(mapper={\n",
       "    queried_result: RunnableLambda(fetch_recent_orders)\n",
       "  })\n",
       "| ChatPromptTemplate(input_variables=['check_purpose', 'queried_result'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\n            You are a robot that generates a confirmation request message based on the check_purpose and queried_result.\\n            create a message to show the queried_result and ask for final confirmation considering check_purpose.\\n            The response should be generated in Korean.\\n            ')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['check_purpose', 'queried_result'], template='check_purpose:{check_purpose}\\nqueried_result:{queried_result}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E6568610>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E6568BB0>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_chain2 = (\n",
    "    classify_chain_with_memory | \n",
    "    RunnablePassthrough.assign(check_purpose=classify_chain_with_memory) | \n",
    "    RunnablePassthrough.assign(queried_result=RunnableLambda(fetch_recent_orders)) |\n",
    "    generate_confirm_message_chain\n",
    "    )\n",
    "test_chain2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 94f73d1e-cd00-49dd-bce4-fb2967cf08bc not found for run 3bbec3cf-a86f-4ed7-bdc7-ba980409a60e. Treating as a root run.\n",
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The input to RunnablePassthrough.assign() must be a dict.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_chain2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morder_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigurable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msession_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_240516-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py:469\u001b[0m, in \u001b[0;36mRunnableAssign.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    466\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    468\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:1626\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1623\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[0;32m   1624\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1625\u001b[0m         Output,\n\u001b[1;32m-> 1626\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1627\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1629\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1630\u001b[0m             config,\n\u001b[0;32m   1631\u001b[0m             run_manager,\n\u001b[0;32m   1632\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1633\u001b[0m         ),\n\u001b[0;32m   1634\u001b[0m     )\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1636\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\passthrough.py:450\u001b[0m, in \u001b[0;36mRunnableAssign._invoke\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_invoke\u001b[39m(\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28minput\u001b[39m: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    449\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 450\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m\n\u001b[0;32m    452\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    456\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapper\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    460\u001b[0m         ),\n\u001b[0;32m    461\u001b[0m     }\n",
      "\u001b[1;31mAssertionError\u001b[0m: The input to RunnablePassthrough.assign() must be a dict."
     ]
    }
   ],
   "source": [
    "test_chain2.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 확인 요청 메시지 작성 이후 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 28438fb1-5617-465c-84f5-46f8bb2e1502 not found for run 4280a95f-4268-45d5-be10-30dff4d1c810. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'order_id': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got 3.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='주문 내용을 확인해보실래요?\\n\\n상품: 백설기\\n수량: 2개\\n가격: 13,000원\\n\\n주문 변경을 원하시나요? 최종 확인 부탁드립니다.', response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 185, 'total_tokens': 251}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-215495d7-2c89-4565-8cdd-cdb7b4a3bbf0-0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_chain1.invoke(\n",
    "    {\"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요청 확인 부분 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "test_chain1 = {\n",
    "    \"order_id\": RunnablePassthrough(),\n",
    "    \"check_purpose\": classify_chain_with_memory,\n",
    "    \"queried_result\": RunnableLambda(fetch_recent_orders)\n",
    "} | generate_confirm_message_chain\n",
    "test_chain1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory 공유 확인 위한 dummy chain 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "dummy_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types. If order_id is provided, you need to output '조회'.\n",
    "            If order_id is not provided, you need to classify the customer input message into one of the following two types:\n",
    "            - 상품 문의, 주문 내역 조회, 주문 변경 내역 조회, 주문 취소 내역 조회: '문의'\n",
    "            - 주문 요청, 주문 변경 요청, 주문 취소 요청: '요청'\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"input:{input}\\norder_id:{order_id}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_chain = dummy_prompt | model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for RunnableWithMessageHistory\ninput_messages_key\n  str type expected (type=type_error.str)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnableWithMessageHistory\n\u001b[1;32m----> 3\u001b[0m dumy_chain_with_memory \u001b[38;5;241m=\u001b[39m \u001b[43mRunnableWithMessageHistory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdummy_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_session_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_messages_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morder_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory_messages_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\history.py:328\u001b[0m, in \u001b[0;36mRunnableWithMessageHistory.__init__\u001b[1;34m(self, runnable, get_session_history, input_messages_key, output_messages_key, history_messages_key, history_factory_config, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;66;03m# If not provided, then we'll use the default session_id field\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     _config_specs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    318\u001b[0m         ConfigurableFieldSpec(\n\u001b[0;32m    319\u001b[0m             \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    325\u001b[0m         ),\n\u001b[0;32m    326\u001b[0m     ]\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    329\u001b[0m     get_session_history\u001b[38;5;241m=\u001b[39mget_session_history,\n\u001b[0;32m    330\u001b[0m     input_messages_key\u001b[38;5;241m=\u001b[39minput_messages_key,\n\u001b[0;32m    331\u001b[0m     output_messages_key\u001b[38;5;241m=\u001b[39moutput_messages_key,\n\u001b[0;32m    332\u001b[0m     bound\u001b[38;5;241m=\u001b[39mbound,\n\u001b[0;32m    333\u001b[0m     history_messages_key\u001b[38;5;241m=\u001b[39mhistory_messages_key,\n\u001b[0;32m    334\u001b[0m     history_factory_config\u001b[38;5;241m=\u001b[39m_config_specs,\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    336\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:4450\u001b[0m, in \u001b[0;36mRunnableBindingBase.__init__\u001b[1;34m(self, bound, kwargs, config, config_factories, custom_input_type, custom_output_type, **other_kwargs)\u001b[0m\n\u001b[0;32m   4422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m   4423\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4424\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4433\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_kwargs: Any,\n\u001b[0;32m   4434\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a RunnableBinding from a runnable and kwargs.\u001b[39;00m\n\u001b[0;32m   4436\u001b[0m \n\u001b[0;32m   4437\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4448\u001b[0m \u001b[38;5;124;03m        **other_kwargs: Unpacked into the base class.\u001b[39;00m\n\u001b[0;32m   4449\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4450\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   4451\u001b[0m         bound\u001b[38;5;241m=\u001b[39mbound,\n\u001b[0;32m   4452\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m   4453\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m   4454\u001b[0m         config_factories\u001b[38;5;241m=\u001b[39mconfig_factories \u001b[38;5;129;01mor\u001b[39;00m [],\n\u001b[0;32m   4455\u001b[0m         custom_input_type\u001b[38;5;241m=\u001b[39mcustom_input_type,\n\u001b[0;32m   4456\u001b[0m         custom_output_type\u001b[38;5;241m=\u001b[39mcustom_output_type,\n\u001b[0;32m   4457\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_kwargs,\n\u001b[0;32m   4458\u001b[0m     )\n\u001b[0;32m   4459\u001b[0m     \u001b[38;5;66;03m# if we don't explicitly set config to the TypedDict here,\u001b[39;00m\n\u001b[0;32m   4460\u001b[0m     \u001b[38;5;66;03m# the pydantic init above will strip out any of the \"extra\"\u001b[39;00m\n\u001b[0;32m   4461\u001b[0m     \u001b[38;5;66;03m# fields even though total=False on the typed dict.\u001b[39;00m\n\u001b[0;32m   4462\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[1;32mc:\\Users\\Tae-su\\Grow\\repositories\\orderbot_v2\\venv\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for RunnableWithMessageHistory\ninput_messages_key\n  str type expected (type=type_error.str)"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "dumy_chain_with_memory = RunnableWithMessageHistory(\n",
    "    dummy_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 710052a1-b37d-455c-bbe6-687714c6eafe not found for run f0cb69af-44d7-4ae9-b080-8c74ce697107. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 150, 'total_tokens': 153}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b68da99e-cdad-4e22-ae65-06f52fc132fc-0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = dumy_chain_with_memory.invoke(\n",
    "    {\"input\": \"주문 취소하고 싶어\", \n",
    "    \"order_id\": None},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 0a619675-f802-47b4-9663-56b4d0b6222a not found for run 2006defa-a6cf-45a6-a7fe-567c37196141. Treating as a root run.\n",
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got None.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='조회', response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 162, 'total_tokens': 164}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bd1c1362-266f-47f9-88aa-2379272c82fb-0')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = dumy_chain_with_memory.invoke(\n",
    "    {\"input\": None, \n",
    "    \"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 취소하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 150, 'total_tokens': 153}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b68da99e-cdad-4e22-ae65-06f52fc132fc-0')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240516-1\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프롬프트 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['\"check_purpose\"', 'chat_history', 'input', 'order_id'] input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['\"check_purpose\"'], template='\\n            You are a robot that classifies customer input messages into specific types.\\n            You need to review the conversation from the latest to the oldest and output either \\'주문 변경\\' or \\'주문 취소\\'.\\n            After classifying into \\'주문 변경\\' or \\'주문 취소\\', you need to determine whether the user has agreed to the action (yes or no) based on the recent conversation and user input.\\n            Respond with a JSON object in the following format:\\n            {\"check_purpose\": \"<주문 변경 or 주문 취소>\", \"confirmation\": \"<yes or no>\"}\\n            ')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'order_id'], template='input: {input}\\norder_id:{order_id}'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "order_change_cancel_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types.\n",
    "            You need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\n",
    "            After classifying into '주문 변경' or '주문 취소', you need to determine whether the user has agreed to the action (yes or no) based on the recent conversation and user input.\n",
    "            Respond with a JSON object in the following format:\n",
    "            {\"check_purpose\": \"<주문 변경 or 주문 취소>\", \"confirmation\": \"<yes or no>\"}\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"input: {input}\\norder_id:{order_id}\"),\n",
    "    ]\n",
    ")\n",
    "print(order_change_cancel_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "order_change_cancel_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a robot that classifies customer input messages into specific types.\n",
    "            You need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\n",
    "            After classifying into '주문 변경' or '주문 취소', you need to determine whether the user has agreed to the action (yes or no) based on the recent conversation and user input.\n",
    "            Respond with a JSON object in the following format:\n",
    "            \"check_purpose\": \"<주문 변경 or 주문 취소>\", \"confirmation\": \"<yes or no>\"\n",
    "            \"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"order_id:{order_id}\\ninput:{input}\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input', 'order_id'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\n            You are a robot that classifies customer input messages into specific types.\\n            You need to review the conversation from the latest to the oldest and output either \\'주문 변경\\' or \\'주문 취소\\'.\\n            After classifying into \\'주문 변경\\' or \\'주문 취소\\', you need to determine whether the user has agreed to the action (yes or no) based on the recent conversation and user input.\\n            Respond with a JSON object in the following format:\\n            \"check_purpose\": \"<주문 변경 or 주문 취소>\", \"confirmation\": \"<yes or no>\"\\n            ')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input', 'order_id'], template='order_id:{order_id}\\ninput:{input}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E68418A0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E8480340>, openai_api_key=SecretStr('**********'), openai_proxy='')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_chain = order_change_cancel_prompt | model\n",
    "classify_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "test_chain3 = RunnableWithMessageHistory(\n",
    "    classify_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 취소하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 150, 'total_tokens': 153}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b68da99e-cdad-4e22-ae65-06f52fc132fc-0')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240516-1\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 0d19f63c-4613-4cf3-ac37-2af962ec012d not found for run 50dfb1fe-0f57-4279-bfaf-203c75b519c7. Treating as a root run.\n",
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got None.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='주문 취소, 확인하시겠습니까?', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 166, 'total_tokens': 182}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-96fa79c7-e88f-4e79-ade0-5f65aea2bc1d-0')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = test_chain3.invoke(\n",
    "    {\"input\": None, \n",
    "    \"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['chat_history', 'input', 'order_id'], partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"check_purpose\": {\"description\": \"\\\\uc8fc\\\\ubb38 \\\\ubcc0\\\\uacbd or \\\\uc8fc\\\\ubb38 \\\\ucde8\\\\uc18c\", \"title\": \"Check Purpose\", \"type\": \"string\"}, \"confirmation\": {\"description\": \"yes or no\", \"title\": \"Confirmation\", \"type\": \"string\"}}, \"required\": [\"check_purpose\", \"confirmation\"]}\\n```'}, template=\"\\n        You are a robot that classifies customer input messages into specific types.\\n        You need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n        \\n        After classifying into '주문 변경' or '주문 취소', you need to check if there is an AI message showing the order details for cancellation confirmation, and whether the user agreed to this cancellation confirmation message.\\n        Based on this, determine the 'confirmation' field value as 'yes' if both conditions are satisfied.\\n        \\n        chat_history: {chat_history}\\n        customer_input: {input}\\n        order_id: {order_id}\\n        \\n        chat_history: {chat_history}\\n        customer_input: {input}\\n        order_id: {order_id}\\n        \\n        {format_instructions}\\n    \")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define your desired data structure.\n",
    "class OrderChangeCancelConfirmation(BaseModel):\n",
    "    check_purpose: str = Field(description=\"주문 변경 or 주문 취소\")\n",
    "    confirmation: str = Field(description=\"yes or no\")\n",
    "\n",
    "# Set up a parser.\n",
    "parser = JsonOutputParser(pydantic_object=OrderChangeCancelConfirmation)\n",
    "\n",
    "# Create the prompt.\n",
    "order_change_cancel_prompt = PromptTemplate(\n",
    "    template=(\"\"\"\n",
    "        You are a robot that classifies customer input messages into specific types.\n",
    "        You need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\n",
    "        \n",
    "        After classifying into '주문 변경' or '주문 취소', you need to check if there is an AI message showing the order details for cancellation confirmation, and whether the user agreed to this cancellation confirmation message.\n",
    "        Based on this, determine the 'confirmation' field value as 'yes' if both conditions are satisfied.\n",
    "        \n",
    "        chat_history: {chat_history}\n",
    "        customer_input: {input}\n",
    "        order_id: {order_id}\n",
    "        \n",
    "        chat_history: {chat_history}\n",
    "        customer_input: {input}\n",
    "        order_id: {order_id}\n",
    "        \n",
    "        {format_instructions}\n",
    "    \"\"\"),\n",
    "    input_variables=[\"chat_history\", \"order_id\", \"input\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "order_change_cancel_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['chat_history', 'input', 'order_id'], partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"check_purpose\": {\"description\": \"\\\\uc8fc\\\\ubb38 \\\\ubcc0\\\\uacbd or \\\\uc8fc\\\\ubb38 \\\\ucde8\\\\uc18c\", \"title\": \"Check Purpose\", \"type\": \"string\"}, \"confirmation\": {\"description\": \"yes or no\", \"title\": \"Confirmation\", \"type\": \"string\"}}, \"required\": [\"check_purpose\", \"confirmation\"]}\\n```'}, template=\"\\n        You are a robot that classifies customer input messages into specific types.\\n        You need to review the conversation from the latest to the oldest and output either '주문 변경' or '주문 취소'.\\n        \\n        After classifying into '주문 변경' or '주문 취소', you need to check if there is an AI message showing the order details for cancellation confirmation, and whether the user agreed to this cancellation confirmation message.\\n        Based on this, determine the 'confirmation' field value as 'yes' if both conditions are satisfied.\\n        \\n        chat_history: {chat_history}\\n        customer_input: {input}\\n        order_id: {order_id}\\n        \\n        chat_history: {chat_history}\\n        customer_input: {input}\\n        order_id: {order_id}\\n        \\n        {format_instructions}\\n    \")\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x00000235E87264A0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000235E8363280>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')\n",
       "| JsonOutputParser(pydantic_object=<class '__main__.OrderChangeCancelConfirmation'>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_chain = order_change_cancel_prompt | model | parser\n",
    "classify_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "test_chain3 = RunnableWithMessageHistory(\n",
    "    classify_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='주문 취소하고 싶어'),\n",
       " AIMessage(content='요청', response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 150, 'total_tokens': 153}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b68da99e-cdad-4e22-ae65-06f52fc132fc-0')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[\"test_240516-1\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 5fa2fdf9-9058-4ae1-8f7c-ae8d406eeeb5 not found for run cc2b9434-13d3-494c-bb13-394da1b91dbb. Treating as a root run.\n",
      "Error in RootListenersTracer.on_chain_end callback: ValueError('Expected str, BaseMessage, List[BaseMessage], or Tuple[BaseMessage]. Got None.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'check_purpose': '주문 취소', 'confirmation': 'yes'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = test_chain3.invoke(\n",
    "    {\"input\": None, \n",
    "    \"order_id\": 3},\n",
    "    config={\"configurable\": {\"session_id\": \"test_240516-1\"}}\n",
    "    )\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'check_purpose': '주문 취소', 'confirmation': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
